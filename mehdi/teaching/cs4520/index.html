<!DOCTYPE html
  PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
  <link href="style.css" rel="stylesheet" type="text/css" />
  <title>CSCI 4520 - Introduction to Machine Learning</title>
</head>

<body>
  <div id="container">
    <div id="header">
      <div class="titles">
        <h3>
          <span class="uni">Georgia Southern University</span>
        </h3>
        <h3 class="dept">Computer Science Department</h3>
        <hr />
        <h1> CSCI 4520 - Introduction to Machine Learning
          <br /> Spring 2020</h1>

      </div>
      <div class="spacer"></div>
    </div>
    <div id="course">
      <table class="line" cellspacing="10" cellpadding="0" border="0">
        <tbody>
          <tr class="line">
            <td width="150px">
              <strong>Info:</strong>
            </td>
            <td>
              <a href="syllabus.pdf">Description and Syllabus</a>
            </td>
          </tr>
          <tr class="line">
            <td valign="top">
              <strong>Day/Time:</strong>
            </td>
            <td>
              <table border="0">
                <tbody>
                  <tr>
                    <td>Tuesday and Thursday, 12:30 PM - 1:45 PM (CEIT 3208)</td>
                  </tr>
                </tbody>
              </table>
            </td>
          </tr>

          <tr class="line">
            <td>
              <strong>Instructor:</strong>
            </td>
            <td>
              <a href="https://sci2lab.github.io/mehdi/">Mehdi Allahyari</a>
            </td>
          </tr>
          <tr class="line">
            <td>
              <strong>Email:</strong>
            </td>
            <td>mallahyari@georgiasouthern.edu</td>
          </tr>
          <tr class="line">
            <td>
              <strong>Office:</strong>
            <td>CEIT 2321</td>
          </tr>
          <tr class="line">
            <td>
              <strong>Office Hours:</strong>
            </td>
            <td>
              <table style="margin: 10px 0px" border="0">
                <tr>
                  <td>
                    <em>
                      <strong>Monday</strong>
                    </em> through
                    <em>
                      <strong>Thursday</strong></em>:</td>
                  <td>4:00 PM - 5:00 PM</td>
                </tr>

              </table>
            </td>
          </tr>
          <tr class="line">
            <td>
              <strong>TA</strong>
            <td>
              TBA
            </td>
          </tr>
          <tr>
            <td valign="center">
              <strong>Prerequisite:</strong>
            </td>
            <td valign="top" align="left">
              <p>Students entering the class are expected to have a pre-existing working knowledge of probability,
                linear algebra,
                statistics and calculus. For example, it is expected that you know about standard probability
                distributions
                and also how to calculate derivatives. In addition, recitation sessions will be held to review some
                basic
                concepts. For the programming assignments, you should have good background in programming, and it would
                be
                helpful if you know Python.</p>
            </td>
          </tr>
          <tr class="line">
            <td valign="center">
              <b>Text books:
              </b>
            </td>
            <td valign="top" align="left">
              <p><strong>Highly recommanded:</strong></p>
              <ul>
                <li>
                  <a href="https://www.oreilly.com/library/view/introduction-to-machine/9781449369880/"
                    target="_blank">Introduction to Machine Learning with Python</a>, Andreas C. Muller and Sarah Guido.
                  <br />
                  <p>Code examples and figures are freely available <a
                      href="https://github.com/amueller/introduction_to_ml_with_python">here</a> on Github.</p>

                </li>
                <li>
                  <a href="https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/"
                    target="_blank">Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow</a>, Aurelien
                  Geron<strong>(Highly recommanded)</strong> <br />
                  <p>Code examples and figures are freely available <a
                      href="https://github.com/ageron/handson-ml2">here</a> on Github.</p>
                </li>
              </ul>
              <p><strong>NOTE: </strong>Many of the hands-on code examples, topics, and figures discussed in class were
                adopted from the books above; hence, it is highly recommended to read through the chapters in the
                aformentioned books.</p>
              <p><strong>Optional:</strong></p>
              <ul>
                <li>
                  <a href="http://www.cs.cmu.edu/afs/cs.cmu.edu/user/mitchell/ftp/mlbook.html" target="_blank">Machine
                    Learning</a>, Tom Mitchell.

                </li>
                <li>
                  <a href="https://web.stanford.edu/~hastie/ElemStatLearn/" target="_blank"> The Elements of Statistical
                    Learning: Data Mining, Inference and Prediction,</a> Trevor Hastie, Robert Tibshirani, Jerome
                  Friedman
                </li>
              </ul>
            </td>
          </tr>

          <tr class="line">
            <td width="21%" valign="top" style="padding-top:10px;">
              <b>Grading:</b>
            </td>
            <td width="79%" valign="top">
              <table width="40%" border="1" cellpadding="0" cellspacing="0" style="margin-top:10px;">
                <tr>
                  <td style="padding-left: 5px">Attendance &amp; Participation</td>
                  <td align="center">5%</td>
                </tr>
                <tr>
                  <td style="padding-left: 5px">Homeworks</td>
                  <td align="center">20%</td>
                </tr>
                <tr>
                  <td width="79%" style="padding-left: 5px">Midterm</td>
                  <td width="21%" align="center">20%</td>
                </tr>
                <tr>
                  <td width="79%" style="padding-left: 5px">Final Exam</td>
                  <td width="21%" align="center">25%</td>
                </tr>
                <tr>
                  <td width="79%" style="padding-left: 5px">Group Project</td>
                  <td width="21%" align="center">30%</td>
                </tr>
              </table>
              <br>
            </td>
          </tr>
          <tr class="line">
            <td valign="top" style="padding-top:15px;">
              <strong>Grading Scale:</strong>
            </td>
            <td valign="top">
              <table id="gscale" width="150" height="127" border="1" cellpadding="0" cellspacing="0">
                <tr>
                  <td width="50">85-100 </td>
                  <td width="26">A</td>
                </tr>
                <tr>
                  <td>80-85</td>
                  <td>B</td>
                </tr>
                <tr>
                  <td>70-80 </td>
                  <td>C</td>
                </tr>
                <tr>
                  <td>65-70 </td>
                  <td>D</td>
                </tr>
                <tr>
                  <td>&lt; 65</td>
                  <td>F</td>
                </tr>
              </table>
            </td>
          </tr>
          <tr class="line">
            <td width="21%" valign="top">
              <b>Course Description:</b>
            </td>
            <td width="79%" valign="top">
              <p>Machine Learning methods aim to enable computers to automatically learn and improve their performance
                through
                experience (e.g., programs that learn to recognize human faces, recommend music and movies, and detect
                Spam
                emails).
              </p>
              <p> This introductory course is designed to give upperlevel undergraduate a broad overview of many
                concepts and algorithms in ML. The goal is to provide students with a deep understanding of the subject
                matter and skills to apply these concepts to real world problems. This course covers the theory and
                practical algorithms for machine learning from a variety of perspectives. We cover topics such as
                classification, linear and logistic regression, decision tree learning, unsupervised learning,
                clustering and dimensionality reduction. In addition, the course covers fundamental concepts such as
                training, validation, overfitting, and error rates as well as practical use of machine learning
                algorithms using open source libraries from the Python programming ecosystem.</p>
            </td>
          </tr>

          <tr class="line">
            <td valign="top">
              <b>Exam dates:</b>
            </td>
            <td valign="top">
              <table border="0" width="100%">
                <tr>
                  <td width="17%" valign="top">Midterm</td>
                  <td width="24%" valign="top">TBA</td>
                  <td width="59%" valign="top"> regular class time</td>
                </tr>
                <tr>
                  <td width="17%" valign="top">Final&nbsp;</td>
                  <td width="24%" valign="top">05/05/2020</td>
                  <td width="59%" valign="top">12:30 PM - 2:30 PM</td>
                </tr>
              </table>
            </td>
          </tr>

          <!-- <tr class="line">
            <td width="21%" valign="center">
              <b>Final Project:</b>
            </td>
            <td width="79%" valign="top">
              <h3>Important Dates:</h3>
              <ul>
                <li>Wednesday, Fabruary 4, 2018: Project proposals</li>
                <li>Wednesday, April 25, 2018: Project report</li>
              </ul>
              <h3>Project Description</h3>
              <ul>
                <li>Read
                  <a href="finalProject.pdf">course project's instructions</a> for all the details regarding the final project.</li>
              </ul>
              <h3>Notes:</h3>
              <ul>
                <li>Graduate students must work on individual projects</li>
                <li>Graduate students must present their work as a poster at the Student Research Symposium that will be held
                  on Thursday, April 19. Here's the
                  <a href="poster_submission_guide.pdf">link</a> for poster submission guidlines.</li>
                <li>Undergraduate students are expected (encouraged) to present their project at the Student Research Symposium.
                  The groups participating in the poster session will get 5 points as extra credit.</li>
              </ul>

            </td>
          </tr> -->



        </tbody>
      </table>
    </div>

    <div id="finalProject">
      <h2> Final Project</h2>
      <h3>General Guidelines</h3>
      <p>The final project is an opportunity for you to try machine learning techniques on interesting realworld
        problems. What we expect to see is an idea (task) that you clearly describe, implement and
        test on a dataset. Projects shoule be carried out in <strong>teams of two students</strong> You can
        discuss your ideas and approach with the instructor, but obviously the final responsibility to define
        and execute an interesting piece of work is yours. The final project is worth 20% of your grade,
        which will be divided in two deliverables: proposal and final report. </p>
      <p>Final projects will be evaluated by the following three criteria:</p>
      <ol>
        <li><strong>Technical Depth:</strong> How technically challenging was what you did?</li>
        <li> <strong>Scope:</strong> How broad was your project? How many aspects, angles, variations did you explore?
        </li>
        <li> <strong>Presentation:</strong> How well did you explain what you did, your results, and interpret the
          outcomes? Did you use the good graphs and visualizations? How clear was the writing?</li>
      </ol>
      <h3>Project Proposal</h3>
      <p>You must turn in a project proposal (hardcopy and electronic copy) on <strong>February 4th</strong>. The
        hardcopy must be turned in at the beginning of the class and the pdf file on Folio</p>
      <h3>Project Proposal Format</h3>
      <p>Your proposal should be a PDF document, giving the title of the project, and the full names of all of your team
        members, and a 500-800 word description of what you plan to do. Your proposals should include the following
        information:</p>
      <p><strong>Motivation: </strong>What problem you are tackling? Describe why your project is exciting. E.g., you
        can describe why your project could have a broader societal impact. Or, you may describe the motivation from a
        personal learning perspective.</p>
      <p><strong>Method: </strong>What machine learning techniques you plan to apply or improve upon</p>
      <p><strong>Experiemnts: </strong>What experiments you intend to run. How you plan to evaluate your machine
        learning algorithm. What datasets you plan to use.</p>
      <p><strong>References: </strong>Include at least 5 relevant papers. You will probably want to read at least one of
        them before submitting your proposal.</p>
      <h3>Contributions</h3>
      <p>You are expected to share the workload evenly, and every group member should participate in both the
        experiments and writing. </p>
      <h3>Project Ideas</h3>
      <p>You are encouraged come up with your own ideas for projects. For instance, you can look for data
        for a problem that you are interested in and try out different machine learning algorithms to predict
        something interesting about the data. You may also look at the various current and past competitions on <a
          href="https://www.kaggle.com/competitions">Kaggle</a> for inspiration. You are welcome to explore and expand
        upon project ideas listed below or use these as a starting
        point for brainstorming ideas.</p>
      <ul>
        <li>
          courtesy: Andrew Ng, <a href="http://cs229.stanford.edu/projects.html">link</a>
        </li>
        <li>
          courtesy: Carlos Guestrin, <a
            href="https://courses.cs.washington.edu/courses/cse446/13sp/projects.html#proposal">link</a>
        </li>
        <li>
          courtesy: Vibhav Gogat, <a href="http://www.hlt.utdallas.edu/~vgogate/ml/2012s/projects.html">link</a>
        </li>
        <li>
          courtesy: Ilyas Cicekli, <a
            href="https://web.cs.hacettepe.edu.tr/~ilyas/Courses/BIL712/possibleprojecttopics.pdf">link</a>
        </li>
        <li>
          courtesy: Tom Heskes, <a href="http://www.ru.nl/datascience/education/student_projects/">link</a>
        </li>
        <li>
          courtesy: Rob Schapire, <a
            href="http://www.cs.princeton.edu/courses/archive/spr03/cs511/project.html">link</a>
        </li>
        <li>
          courtesy: Jason Brownlee, <a
            href="https://machinelearningmastery.com/self-study-machine-learning-projects/">link</a>
        </li>
      </ul>

    </div> <!-- finalProject-->

    <!-- course div -->
    <div class="schedule">
      <h2>Lectures and Schedules (subject to change)</h2>
      <table class="schtable" border="1">
        <tr>
          <th>Day</th>
          <th>Lecture</th>
          <th>Readings and useful links</th>
          <th>Notes</th>
        </tr>
        <tr>
          <td>Jan 14</td>
          <td>Course Overview
            <br>
            <a href="slides/lecture1.pdf">Introduction to Machine Learning</a>
          </td>
          <td>
            <ul>
              <li><a href="http://www.cs.cmu.edu/%7Etom/pubs/MachineLearning.pdf">The Discipline of Machine
                  Learning</a>, Tom
                Mitchell</li>
              <li><a href="http://www.youtube.com/watch?v=ZT8LszMo0D4">What is Machine Learning?</a>, Bernhard Scholkopf
              </li>
              <li>Muller Ch 1</li>
              <li>Geron: Ch 1</li>
            </ul>
          </td>
          <td></td>
        </tr>
        <tr>
          <td>Jan 16</td>
          <td>
            <a href="slides/lecture2.pdf">Decision Trees</a>
          </td>
          <td>Readings:
            <ul>

              <li>Mitchell: Ch 3</li>
              <li><a
                  href="http://webdocs.cs.ualberta.ca/~aixplore/learning/DecisionTrees/InterArticle/2-DecisionTree.html">Decision
                  Tree Applet</a></li>
              <li>
                <a href="https://developers.google.com/edu/python/">Python tutorial</a>
              </li>

            </ul>

          </td>
          <td>
            <a href="https://github.com/sci2lab/csci4520-s2020/tree/master/hw1">Homework 1</a><br>
            <span style="color: red;"> Due on Jan-31-2020 (9:00 PM)</span>
          </td>
        </tr>
        <tr>
          <td>Jan 23</td>
          <td>
            <a href="slides/lecture3.pdf">Review of Probability</a>
            <ul>
              <li>Random variables, probabilities</li>
              <li>Probability and Estimation</li>
              <li>Bayes rule, MLE, MAP</li>
            </ul>
          </td>
          <td>Readings:
            <ul>
              <li>Mitchell:
                <a href="http://www.cs.cmu.edu/~tom/mlbook/Joint_MLE_MAP.pdf">Estimating Probabilities: MLE and MAP</a>
              </li>
            </ul>
          </td>
          <td>
          </td>
        </tr>
        <tr>
          <td>Jan 30</td>
          <td>
            <a href="slides/lecture4.pdf">Naive Bayes</a>
            <br>
            <ul>
              <li>Conditional Independence</li>
              <li>Naive Bayes: why and how</li>
              <li>Bayesian Learning</li>
            </ul>
          </td>
          <td>Readings:
            <br>
            <ul>
              <li>Mitchell Ch 6</li>
              <li>Mitchell:
                <a href="http://www.cs.cmu.edu/%7Etom/mlbook/NBayesLogReg.pdf">Naive Bayes and Logistic Regression</a>
              </li>
              <li>Text Classification Examples:
                <a href="https://www.inf.ed.ac.uk/teaching/courses/inf2b/learnnotes/inf2b-learn07-notes-nup.pdf">[Link
                  1]</a>,
                <a
                  href="https://www.3pillarglobal.com/insights/document-classification-using-multinomial-naive-bayes-classifier">[Link
                  2]</a>
              </li>
            </ul>
          </td>
          <td>
            <a href="https://github.com/sci2lab/csci4520-s2020/tree/master/hw2">Homework 2</a><br>
            <span style="color: red;"> Due on Feb-12-2020 (9:00 PM)</span>
          </td>
        </tr>
        <tr>
          <td width="60">Feb 4</td>
          <td>
            <a href="slides/lecture5.pdf">Linear Regression</a>
            <br>
            <ul>
              <li>Generative/Discriminative models</li>
              <li>Minimizing squared error and maximizing data likelihood</li>
              <li>Regularization</li>
            </ul>
          </td>
          <td>Readings:
            <br>
            <ul>

              <li>Andrew Ng:
                <a href="http://cs229.stanford.edu/notes/cs229-notes1.pdf">Lecture notes</a>
              </li>
              <li>Siraj Raval:
                <a href="https://www.youtube.com/watch?v=XdM6ER7zTLk">[Video]</a>
              </li>

            </ul>
          </td>
          <td></td>
        </tr>

      </table>
    </div> <!-- course div end-->


    <!-- old course div -->
    <!-- <div class="schedule">
      <h2>Lectures and Schedules (subject to change)</h2>
      <table class="schtable" border="1">
        <tr>
          <th>Week</th>
          <th>Lecture</th>
          <th>Readings and useful links</th>
          <th>Notes</th>
        </tr>
        <tr>
          <td>Jan 08</td>
          <td>Course Overview
            <br>
            <a href="slides/intro-1.pdf">Introduction to Machine Learning</a>
          </td>
          <td>
            <a href="http://www.cs.cmu.edu/%7Etom/pubs/MachineLearning.pdf">The Discipline of Machine Learning</a>, Tom Mitchell
            <br>
            <a href="http://www.youtube.com/watch?v=ZT8LszMo0D4">What is Machine Learning?</a>, Bernhard Scholkopf</td>
          <td></td>
        </tr>
        <tr>
          <td>Jan 15</td>
          <td>
            <a href="slides/decisionTrees-2.pdf">Decision Trees</a>
          </td>
          <td>Readings:
            <ul>
              <li>Mitchell: Ch 3</li>
              <li>Bishop: Ch 14.4</li>
            </ul>
            <a href="http://webdocs.cs.ualberta.ca/~aixplore/learning/DecisionTrees/InterArticle/2-DecisionTree.html">Decision Tree Applet</a>
          </td>
          <td></td>
        </tr>
        <tr>
          <td>Jan 22</td>
          <td>
            <a href="slides/probability-3.pdf">Review of Probability</a>
            <ul>
              <li>Random variables, probabilities</li>
            </ul>
          </td>
          <td>Readings:
            <ul>
              <li>Bishop Ch 1 - 1.2.3</li>
              <li>Bishop Ch 2 - 2.2</li>
              <li>
                <a href="https://www.autonlab.org/_media/tutorials/prob18.pdf">Basic Probability Tutorial</a>, Andrew Moore</li>

            </ul>
            <a href="https://developers.google.com/edu/python/">Python tutorial</a>
            <br>
          </td>
          <td>
            <a href="https://sci2lab.github.io/mehdi/teaching/x90/hw/1/homework1.pdf">Homework 1 OUT</a>
          </td>
        </tr>
        <tr>
          <td>Jan 29</td>
          <td>
            <a href="slides/probability-3.pdf">Probability and Estimation</a>
            <ul>
              <li>Bayes rule, MLE, MAP</li>
            </ul>
          </td>
          <td>Readings:
            <br>
            <ul>
              <li>Mitchell:
                <a href="http://www.cs.cmu.edu/~tom/mlbook/Joint_MLE_MAP.pdf">Estimating Probabilities: MLE and MAP</a>
              </li>
            </ul>
          </td>
          <td></td>
        </tr>
        <tr>
          <td>Feb 05</td>
          <td>
            <a href="slides/naiveBayes-4.pdf">Naive Bayes</a>
            <br>
            <ul>
              <li>Conditional Independence</li>
              <li>Naive Bayes: why and how</li>
            </ul>
          </td>
          <td>Readings:
            <br>
            <ul>
              <li>Mitchell Ch 6</li>
              <li>Mitchell:
                <a href="http://www.cs.cmu.edu/%7Etom/mlbook/NBayesLogReg.pdf">Naive Bayes and Logistic Regression</a>
              </li>
              <li>Text Classification Examples:
                <a href="https://www.inf.ed.ac.uk/teaching/courses/inf2b/learnnotes/inf2b-learn07-notes-nup.pdf">[Link 1]</a>,
                <a href="https://www.3pillarglobal.com/insights/document-classification-using-multinomial-naive-bayes-classifier">[Link 2]</a>
              </li>
            </ul>
          </td>
          <td></td>
        </tr>
        <tr>
          <td>Feb 12</td>
          <td>
            <a href="slides/naiveBayes-4.pdf">Gaussian Naive Bayes</a>
            <br>
            <ul>
              <li>Gaussian Bayes classifiers</li>
              <li>Document Classification</li>
              <li>Brain image classification</li>
            </ul>
          </td>
          <td>Readings:
            <br>
            <ul>
              <li>Mitchell:
                <a href="http://www.cs.cmu.edu/%7Etom/mlbook/NBayesLogReg.pdf">Naive Bayes and Logistic Regression</a>
              </li>
            </ul>
          </td>
          <td>Homework 1 Due
            <br>
            <a href="https://sci2lab.github.io/mehdi/teaching/x90/hw/2/homework2.pdf">Homework 2 OUT</a>
          </td>
        </tr>

        <tr>
          <td>Feb 26</td>
          <td>
            <a href="slides/logisticRegression-5.pdf">Logistic Regression</a>
            <br>
            <ul>
              <li>Logistic Regression: Maximizing Conditional Likelihood</li>
              <li>Gradient descent as a general learning (optimization) method</li>
            </ul>
          </td>
          <td>Readings:
            <br>
            <ul>
              <li>Mitchell:
                <a href="http://www.cs.cmu.edu/%7Etom/mlbook/NBayesLogReg.pdf">Naive Bayes and Logistic Regression</a>
              </li>
              <li>Andrew Ng:
                <a href="http://cs229.stanford.edu/notes/cs229-notes1.pdf">Lecture notes</a>
              </li>
              <li>Ng and Jordan
                <a href="http://papers.nips.cc/paper/2020-on-discriminative-vs-generative-classifiers-a-comparison-of-logistic-regression-and-naive-bayes.pdf">[Paper]</a>
              </li>
              <li>Logistic regression
                <a href="http://www.cs.technion.ac.il/~rani/LocBoost/">[Applet]</a>
              </li>
            </ul>
          </td>
          <td></td>
        </tr>
        <tr>
          <td>Mar 05</td>
          <td>Midterm Review</td>
          <td></td>
          <td>Homework 2 Due</td>
        </tr>
        <tr>
          <td>Mar 07</td>
          <td>Midterm Exam</td>
          <td></td>
          <td>
            <a href="https://sci2lab.github.io/mehdi/teaching/x90/hw/3/homework3.pdf">Homework 3 OUT</a>
          </td>
        </tr>
        <tr>
          <td>Mar 12</td>
          <td>Spring Break</td>
          <td></td>
          <td>No Classes</td>
        </tr>
        <tr>
          <td>Mar 19</td>
          <td>
            <a href="slides/graphicalModels-6_1.pdf">Graphical Models I</a>
            <br>
            <ul>
              <li>Bayesian Networks</li>
              <li>Representing joint distributions with conditional independence assumptions</li>
            </ul>

          </td>
          <td>Readings
            <br>
            <ul>
              <li>
                Bishop chapter 8, through 8.2
              </li>
              <li>
                Bayes Net
                <a href="http://www.cs.cmu.edu/~javabayes/Home/applet.html">[Applet]</a>
              </li>
            </ul>
          </td>
          <td></td>
        </tr>
        <tr>
          <td>Mar 26</td>
          <td>
            <a href="slides/graphicalModels-6_2.pdf">Graphical Models II</a>
            <br>
            <ul>
              <li>Inference</li>
              <li>Learning from fully observed data</li>
              <li>Learning from partially observed data</li>
              <li>Expectation Maximization (EM)</li>
            </ul>
          </td>
          <td>Readings
            <br>
            <ul>
              <li>Pieter Abbeel:
                <a href="http://crcv.ucf.edu/people/faculty/xBorji/www/lecturesML/Bayes/fa13-cs188-lecture-13-1PP.pdf">Bayes Nets-Intro</a>
              </li>
              <li>Pieter Abbeel:
                <a href="http://crcv.ucf.edu/people/faculty/xBorji/www/lecturesML/Bayes/fa13-cs188-lecture-14-1PP.pdf">Bayes Nets-Independence</a>
              </li>
              <li>Zhu:
                <a href="http://crcv.ucf.edu/people/faculty/xBorji/www/lecturesML/MLE/em.pdf">EM</a>
              </li>
            </ul>
          </td>
          <td></td>
        </tr>
        <tr>
          <td>Apr 02</td>
          <td>
            <a href="slides/clustering-7.pdf">Clustering</a>
          </td>
          <td>Readings
            <br>
            <ul>
              <li>Bishop Ch 8</li>
              <li>Mitchell Ch 6</li>
              <li>David Sontag:
                <a href="http://crcv.ucf.edu/people/faculty/xBorji/www/lecturesML/Clustering/lecture14.pdf">Clustering</a>
              </li>
              <li>David Sontag:
                <a href="http://crcv.ucf.edu/people/faculty/xBorji/www/lecturesML/Clustering/lecture15.pdf">Hierarchical Clustering</a>
              </li>
              <li>Andrew More:
                <a href="http://crcv.ucf.edu/people/faculty/xBorji/www/lecturesML/Clustering/kmeans11.pdf">Clustering</a>
              </li>
            </ul>
          </td>
          <td></td>
        </tr>
        <tr>
          <td>Apr 09</td>
          <td>Ensemble Methods:
            <a href="slides/bagging_boosting.pdf">Bagging & Boosting</a>
            <br> (By V and Scott)
          </td>
          <td></td>
          <td></td>
        </tr>
        <tr>
          <td>Apr 16</td>
          <td>
            <a href="slides/neuralNetworks-7_1.pdf">Neural Networks I</a>
            <ul>
              <li>Gradient Descent</li>
              <li>Learning of representations</li>
            </ul>
          </td>
          <td>Readings
            <ul>
              <li>Mitchell
                <a href="http://www.cs.cmu.edu/afs/cs.cmu.edu/project/theo-20/www/mlbook/ch4.pdf">Chapter 4</a>
              </li>
              <li>Bishop Ch 5</li>
              <li>Raquel Urtasun:
                <a href="http://www.cs.toronto.edu/~urtasun/courses/CSC411_Fall16/10_nn1.pdf"> Neural Networks</a>
              </li>
            </ul>
          </td>
          <td></td>
        </tr>
        <tr>
          <td>Apr 23</td>
          <td>
            <a href="slides/neuralNetworks-7_2.pdf">Neural Networks II</a>
            <ul>
              <li>Backpropagation</li>
            </ul>
          </td>
          <td>Readings
            <ul>
              <li>
                Backpropagation
                <a href="https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/"> Example: A step by step</a>
              </li>
              <li>
                <a href="https://ayearofai.com/rohan-lenny-1-neural-networks-the-backpropagation-algorithm-explained-abf4609d4f9d">Backpropagation explained</a>
              </li>
            </ul>
          </td>
          <td>
            <a href="https://sci2lab.github.io/mehdi/teaching/x90/hw/4/homework4.pdf">Homework 4 OUT</a>
          </td>
        </tr>
        <tr>
          <td>Apr 25</td>
          <td>Final Projects Presentations:
            <ul>
              <li>Turn in the hard copy of reports</li>
              <li>Present the idea and experiments briefly (5-8 mins) </li>
              <li>Submit the electronic verion of reports to Folio</li>
            </ul>
          </td>
          <td></td>
          <td></td>
        </tr>


      </table>
    </div> -->



  </div>
</body>

</html>
